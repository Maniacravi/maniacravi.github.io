[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Manikandan (Mani) Ravi",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Human Activity Detection using UCI’s HAR Dataset\n\n\n\nIMU\n\nGait\n\nHuman Activity\n\nMachine Learning\n\nCNN\n\n\n\nClassifying Accelerometer data to various activities of daily living (ADL) using Deep Learning.\n\n\n\n\n\nSep 25, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/human-activity-detection/index.html",
    "href": "projects/human-activity-detection/index.html",
    "title": "Human Activity Detection using UCI’s HAR Dataset",
    "section": "",
    "text": "Okay, now we are stepping things up a little bit. This time, the data comes from UCI. According to the website:\nThe dataset is a bit different - it’s already been divided into train and test sets but they are all text files. Once again, let’s get into it - for fun, let’s also use the same model from last time to see if that architecture still works fine for this dataset. If it doesn’t, let’s see if we can build a different one."
  },
  {
    "objectID": "projects/human-activity-detection/index.html#load-data",
    "href": "projects/human-activity-detection/index.html#load-data",
    "title": "Human Activity Detection using UCI’s HAR Dataset",
    "section": "Load Data",
    "text": "Load Data\nThe data is split into total, body and gyro - these are the sensor values. There’s also another dataset that has some 561 features extracted from this. Might be fun to play with them but for now, let’s focus on building our CNN network - so this ‘total’ values would be fine for now. May play with .body’ accel values later.\n\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/total_acc_x_train.txt' # Update this path to your file location\n\n# Load into pandas\nimport pandas as pd\nimport re\n\ndata = pd.read_csv(file_location, sep='\\s+', header=None)\ndisplay(data.head())\ndata.shape\n\n&lt;&gt;:7: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:7: SyntaxWarning: invalid escape sequence '\\s'\n/var/folders/q7/ftl_yg4n1gndbwlnk654hq1h0000gn/T/ipykernel_91706/3576027909.py:7: SyntaxWarning: invalid escape sequence '\\s'\n  data = pd.read_csv(file_location, sep='\\s+', header=None)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n\n\n\n\n0\n1.012817\n1.022833\n1.022028\n1.017877\n1.023680\n1.016974\n1.017746\n1.019263\n1.016417\n1.020745\n...\n1.020981\n1.018065\n1.019638\n1.020017\n1.018766\n1.019815\n1.019290\n1.018445\n1.019372\n1.021171\n\n\n1\n1.018851\n1.022380\n1.020781\n1.020218\n1.021344\n1.020522\n1.019790\n1.019216\n1.018307\n1.017996\n...\n1.019291\n1.019258\n1.020736\n1.020950\n1.020491\n1.018685\n1.015660\n1.014788\n1.016499\n1.017849\n\n\n2\n1.023127\n1.021882\n1.019178\n1.015861\n1.012893\n1.016451\n1.020331\n1.020266\n1.021759\n1.018649\n...\n1.020304\n1.021516\n1.019417\n1.019312\n1.019448\n1.019434\n1.019916\n1.021041\n1.022935\n1.022019\n\n\n3\n1.017682\n1.018149\n1.019854\n1.019880\n1.019121\n1.020479\n1.020595\n1.016340\n1.010611\n1.009013\n...\n1.021295\n1.022934\n1.022183\n1.021637\n1.020598\n1.018887\n1.019161\n1.019916\n1.019602\n1.020735\n\n\n4\n1.019952\n1.019616\n1.020933\n1.023061\n1.022242\n1.020867\n1.021939\n1.022300\n1.022302\n1.022254\n...\n1.022687\n1.023670\n1.019899\n1.017381\n1.020389\n1.023884\n1.021753\n1.019425\n1.018896\n1.016787\n\n\n\n\n5 rows × 128 columns\n\n\n\n(7352, 128)\n\n\n\n# Let's see how the labels look like\n\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/y_train.txt'\n\n# Load\ndata = pd.read_csv(file_location, sep='\\s+', header=None)\ndisplay(data.head())\ndata.shape\n\n&lt;&gt;:6: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:6: SyntaxWarning: invalid escape sequence '\\s'\n/var/folders/q7/ftl_yg4n1gndbwlnk654hq1h0000gn/T/ipykernel_91706/3965665900.py:6: SyntaxWarning: invalid escape sequence '\\s'\n  data = pd.read_csv(file_location, sep='\\s+', header=None)\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n5\n\n\n1\n5\n\n\n2\n5\n\n\n3\n5\n\n\n4\n5\n\n\n\n\n\n\n\n(7352, 1)\n\n\nOkay looks like there are 7352 segments of windowed data with associated labels\n\nMake a Dataframe\n\n# Load X axis data\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/total_acc_x_train.txt'\ndf_total = pd.read_csv(file_location, sep='\\s+', header=None)\nfile_loc_body = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/body_acc_x_train.txt'\ndf_body = pd.read_csv(file_loc_body, sep='\\s+', header=None)\nfile_loc_gyro = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/body_gyro_x_train.txt'\ndf_gyro = pd.read_csv(file_loc_gyro, sep='\\s+', header=None)\n\n# Append Y axis data\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/total_acc_y_train.txt'\ndf_total = pd.concat([df_total, pd.read_csv(file_location, sep='\\s+', header=None)], axis=1)\nfile_loc_body = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/body_acc_y_train.txt'\ndf_body = pd.concat([df_body, pd.read_csv(file_loc_body, sep='\\s+', header=None)], axis=1)\nfile_loc_gyro = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/body_gyro_y_train.txt'\ndf_gyro = pd.concat([df_gyro, pd.read_csv(file_loc_gyro, sep='\\s+', header=None)], axis=1)\n\n# Z axis\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/total_acc_z_train.txt'\ndf_total = pd.concat([df_total, pd.read_csv(file_location, sep='\\s+', header=None)], axis=1)\nfile_loc_body = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/body_acc_z_train.txt'\ndf_body = pd.concat([df_body, pd.read_csv(file_loc_body, sep='\\s+', header=None)], axis=1)\nfile_loc_gyro = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/body_gyro_z_train.txt'\ndf_gyro = pd.concat([df_gyro, pd.read_csv(file_loc_gyro, sep='\\s+', header=None)], axis=1)\n\n# Labels\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/y_train.txt'\ny_train = pd.read_csv(file_location, sep='\\s+', header=None)\n# Subtract 1 from values\ny_train -= 1\n\n\ndf_total.shape, df_body.shape, y_train.shape\n\n((7352, 384), (7352, 384), (7352, 1))\n\n\n\n# Load into 3 dimensional numpy arrays\n\nimport numpy as np\n\nX_total_train = df_total.values.reshape(7352, 128, 3)\nX_body_train = df_body.values.reshape(7352, 128, 3)\nX_gyro_train = df_gyro.values.reshape(7352, 128, 3)\ny_train = np.array(y_train)\n\n\n# Do the same for test data\n\n# Load X axis data\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/total_acc_x_test.txt'\ndf_total_test = pd.read_csv(file_location, sep='\\s+', header=None)\nfile_loc_body = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/body_acc_x_test.txt'\ndf_body_test = pd.read_csv(file_loc_body, sep='\\s+', header=None)\nfile_loc_gyro = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/body_gyro_x_test.txt'\ndf_gyro = pd.read_csv(file_loc_gyro, sep='\\s+', header=None)\n\n# Append Y axis data\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/total_acc_y_test.txt'\ndf_total_test = pd.concat([df_total_test, pd.read_csv(file_location, sep='\\s+', header=None)], axis=1)\nfile_loc_body = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/body_acc_y_test.txt'\ndf_body_test = pd.concat([df_body_test, pd.read_csv(file_loc_body, sep='\\s+', header=None)], axis=1)\nfile_loc_gyro = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/body_gyro_y_test.txt'\ndf_gyro = pd.concat([df_gyro, pd.read_csv(file_loc_gyro, sep='\\s+', header=None)], axis=1)\n\n# Z axis\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/total_acc_z_test.txt'\ndf_total_test = pd.concat([df_total_test, pd.read_csv(file_location, sep='\\s+', header=None)], axis=1)\nfile_loc_body = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/body_acc_z_test.txt'\ndf_body_test = pd.concat([df_body_test, pd.read_csv(file_loc_body, sep='\\s+', header=None)], axis=1)\nfile_loc_gyro = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/body_gyro_z_test.txt'\ndf_gyro = pd.concat([df_gyro, pd.read_csv(file_loc_gyro, sep='\\s+', header=None)], axis=1)\n\n# Labels\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/y_test.txt'\ny_test = pd.read_csv(file_location, sep='\\s+', header=None)\ny_test -= 1\n\n# Convert to numpy arrays\nX_total_test = df_total_test.values.reshape(len(df_total_test), 128, 3)\nX_body_test = df_body_test.values.reshape(len(df_body_test), 128, 3)\nX_gyro_test = df_gyro.values.reshape(len(df_gyro), 128, 3)\ny_test = np.array(y_test)\n\nprint(\"Shapes of test data:\")\nprint(\"X_total_test:\", X_total_test.shape)\nprint(\"X_body_test:\", X_body_test.shape)\nprint(\"X_gyro_test:\", X_gyro_test.shape)\nprint(\"y_test:\", y_test.shape)\n\nOKay, now we have the data. Let’s get to work on building dataloaders and the models"
  },
  {
    "objectID": "projects/human-activity-detection/index.html#build-model",
    "href": "projects/human-activity-detection/index.html#build-model",
    "title": "Human Activity Detection using UCI’s HAR Dataset",
    "section": "Build Model",
    "text": "Build Model\n\nData Loader\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Dataset wrapper\nclass UCIDatasetWrapper(Dataset):\n  def __init__(self, X, y):\n    # Convert (N, T, C) -&gt; (N, C, T) for Conv1D\n    self.X = torch.tensor(X.transpose(0,2,1), dtype=torch.float32)\n    self.y = torch.tensor(y, dtype=torch.long)\n\n  def __len__(self):\n    return len(self.y)\n\n  def __getitem__(self, i):\n    return self.X[i], self.y[i]\n\n\n  train_ds = UCIDatasetWrapper(X_total_train, y_train)\n  train_ds_body = UCIDatasetWrapper(X_body_train, y_train)\n  test_ds = UCIDatasetWrapper(X_total_test, y_test)\n  test_ds_body = UCIDatasetWrapper(X_body_test, y_test)\n\n  train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n  train_dl_body = DataLoader(train_ds_body, batch_size=32, shuffle=True)\n  test_dl = DataLoader(test_ds, batch_size=32, shuffle=False)\n\n\n\nModel\nLets use the same model from the previous notbook\n\n# 1D CNN model (Same as our previous notebook)\nimport torch.nn.functional as F\nclass HAR1DCNN(nn.Module):\n    def __init__(self, in_channels=3, n_classes=6):  # adjust n_classes!\n        super().__init__()\n        self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm1d(32)\n        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.pool = nn.MaxPool1d(2)\n        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Linear(128, n_classes)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.pool(x)          # (N, 32, 20)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)          # (N, 64, 10)\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.global_pool(x).squeeze(-1)  # (N, 128)\n        return self.fc(x)"
  },
  {
    "objectID": "projects/human-activity-detection/index.html#train-the-model",
    "href": "projects/human-activity-detection/index.html#train-the-model",
    "title": "Human Activity Detection using UCI’s HAR Dataset",
    "section": "Train the model",
    "text": "Train the model\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device.type != \"cuda\":\n  device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n\nn_classes = len(torch.unique(torch.tensor(y_train)))\nmodel = HAR1DCNN(in_channels=3, n_classes=n_classes).to(device)\n\n\n# Optimizer and loss function\noptim = torch.optim.Adam(model.parameters(), lr=1e-3)\nloss_fn = nn.CrossEntropyLoss()\n\n\ndef run_epoch(loader, train=True):\n  if train:\n    model.train()\n  else:\n    model.eval()\n  total_loss, correct, total = 0, 0, 0\n  for Xb, yb in loader:\n    Xb, yb = Xb.to(device), yb.to(device)\n    if train:\n      optim.zero_grad()\n    out = model(Xb)\n    loss = loss_fn(out, yb.squeeze()) # Removed - 1\n    if train:\n      loss.backward()\n      optim.step()\n\n    total_loss += loss.item() * Xb.size(0)\n    preds = out.argmax(1)\n\n    correct += (preds == yb.squeeze()).sum().item() # Removed - 1\n    total += Xb.size(0)\n  return total_loss/total, correct/total\n\n\nfor epoch in range(10):\n  train_loss, train_acc = run_epoch(train_dl)\n  test_loss, test_acc = run_epoch(test_dl, train=False)\n  print(f'Epoch {epoch+1}: train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, test_loss={test_loss:.4f}, test_acc={test_acc:.4f}')\n\nEpoch 1: train_loss=0.5972, train_acc=0.8224, test_loss=0.3346, test_acc=0.8748\nEpoch 2: train_loss=0.2438, train_acc=0.9159, test_loss=0.3157, test_acc=0.8812\nEpoch 3: train_loss=0.1846, train_acc=0.9325, test_loss=0.2898, test_acc=0.8836\nEpoch 4: train_loss=0.1789, train_acc=0.9317, test_loss=0.4101, test_acc=0.8422\nEpoch 5: train_loss=0.1512, train_acc=0.9385, test_loss=0.2492, test_acc=0.9189\nEpoch 6: train_loss=0.1553, train_acc=0.9380, test_loss=0.2193, test_acc=0.9237\nEpoch 7: train_loss=0.1452, train_acc=0.9400, test_loss=0.2641, test_acc=0.8985\nEpoch 8: train_loss=0.1344, train_acc=0.9440, test_loss=0.3449, test_acc=0.8660\nEpoch 9: train_loss=0.1375, train_acc=0.9437, test_loss=0.8128, test_acc=0.7842\nEpoch 10: train_loss=0.1337, train_acc=0.9438, test_loss=0.3011, test_acc=0.8911"
  },
  {
    "objectID": "projects/human-activity-detection/index.html#evaluate-model",
    "href": "projects/human-activity-detection/index.html#evaluate-model",
    "title": "Human Activity Detection using UCI’s HAR Dataset",
    "section": "Evaluate model",
    "text": "Evaluate model\n\n# Evaluate the model - get F1 score, confusion matrix\n\nfrom sklearn.metrics import f1_score, confusion_matrix\n\nmodel.eval()\ny_pred = []\ny_true = []\nwith torch.no_grad():\n    for Xb, yb in test_dl:\n        Xb, yb = Xb.to(device), yb.to(device)\n        out = model(Xb)\n        preds = out.argmax(1)\n        y_pred.extend(preds.cpu().numpy())\n        y_true.extend(yb.cpu().numpy())\n\nf1 = f1_score(y_true, y_pred, average='macro')\ncm = confusion_matrix(y_true, y_pred)\nprint(f'F1 score: {f1:.4f}')\n\n# PLot confusion matrix\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title(f'Confusion Matrix (F1 score= {f1:.4f})')\nplt.xticks([0.5, 1.5, 2.5, 3.5, 4.5, 5.5], ['Walking', 'Upstairs', 'Downstairs', 'Sitting', 'Standing', 'Laying'])\nplt.yticks([0.5, 1.5, 2.5, 3.5, 4.5, 5.5], ['Walking', 'Upstairs', 'Downstairs', 'Sitting', 'Standing', 'Laying]'])\nplt.show()\n\nF1 score: 0.8911"
  },
  {
    "objectID": "projects/human-activity-detection/index.html#changing-data",
    "href": "projects/human-activity-detection/index.html#changing-data",
    "title": "Human Activity Detection using UCI’s HAR Dataset",
    "section": "Changing Data",
    "text": "Changing Data\nWhat if we train the model just with the ‘body’ accelerometer data?\nJust to recap: &gt; The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.\n\nfor epoch in range(10):\n  train_loss, train_acc = run_epoch(train_dl_body)\n  test_loss, test_acc = run_epoch(test_dl, train=False)\n  print(f'Epoch {epoch+1}: train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, test_loss={test_loss:.4f}, test_acc={test_acc:.4f}')\n\nEpoch 1: train_loss=0.7195, train_acc=0.6013, test_loss=5.6029, test_acc=0.4167\nEpoch 2: train_loss=0.6618, train_acc=0.6265, test_loss=7.7905, test_acc=0.2131\nEpoch 3: train_loss=0.6509, train_acc=0.6330, test_loss=10.3415, test_acc=0.1904\nEpoch 4: train_loss=0.6354, train_acc=0.6383, test_loss=11.1202, test_acc=0.1982\nEpoch 5: train_loss=0.6318, train_acc=0.6481, test_loss=11.9602, test_acc=0.1890\nEpoch 6: train_loss=0.6207, train_acc=0.6428, test_loss=9.3616, test_acc=0.2185\nEpoch 7: train_loss=0.6265, train_acc=0.6462, test_loss=12.9440, test_acc=0.1822\nEpoch 8: train_loss=0.6175, train_acc=0.6502, test_loss=8.5913, test_acc=0.2151\nEpoch 9: train_loss=0.6117, train_acc=0.6553, test_loss=14.4816, test_acc=0.1890\nEpoch 10: train_loss=0.6114, train_acc=0.6526, test_loss=9.9407, test_acc=0.2711\n\n\n\n# Evaluate the model - get F1 score, confusion matrix\n\nfrom sklearn.metrics import f1_score, confusion_matrix\n\nmodel.eval()\ny_pred = []\ny_true = []\nwith torch.no_grad():\n    for Xb, yb in test_dl:\n        Xb, yb = Xb.to(device), yb.to(device)\n        out = model(Xb)\n        preds = out.argmax(1)\n        y_pred.extend(preds.cpu().numpy())\n        y_true.extend(yb.cpu().numpy())\n\nf1 = f1_score(y_true, y_pred, average='macro')\ncm = confusion_matrix(y_true, y_pred)\nprint(f'F1 score: {f1:.4f}')\n\n# PLot confusion matrix\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title(f'Confusion Matrix (F1 score= {f1:.4f})')\nplt.xticks([0.5, 1.5, 2.5, 3.5, 4.5, 5.5], ['Walking', 'Upstairs', 'Downstairs', 'Sitting', 'Standing', 'Laying'])\nplt.yticks([0.5, 1.5, 2.5, 3.5, 4.5, 5.5], ['Walking', 'Upstairs', 'Downstairs', 'Sitting', 'Standing', 'Laying]'])\nplt.show()\n\nF1 score: 0.1747\n\n\n\n\n\n\n\n\n\nAs we can see (and intutitively expect), if we take out the gravitational component, the accuracy values are pretty poor. What if we feed all these 9 vectors in?"
  },
  {
    "objectID": "projects/human-activity-detection/index.html#running-the-model-with-all-the-data",
    "href": "projects/human-activity-detection/index.html#running-the-model-with-all-the-data",
    "title": "Human Activity Detection using UCI’s HAR Dataset",
    "section": "Running the model with all the data",
    "text": "Running the model with all the data\n\n# Concatenate total, body and gyro\n\nX_all_train = np.concat((X_total_train, X_body_train, X_gyro_train), axis=2)\ndisplay(X_all_train.shape, y_train.shape) # We should have 9 channels now\nX_all_test = np.concat((X_total_test, X_body_test, X_gyro_test), axis=2)\ndisplay(X_all_test.shape, y_test.shape)\n\n(7352, 128, 9)\n\n\n(7352, 1)\n\n\n(2947, 128, 9)\n\n\n(2947, 1)\n\n\n\n# Create data loaders\n\ntrain_ds_all = UCIDatasetWrapper(X_all_train, y_train)\ntrain_dl_all = DataLoader(train_ds_all, batch_size=32, shuffle=True)\ntest_ds_all = UCIDatasetWrapper(X_all_test, y_test)\ntest_dl_all = DataLoader(test_ds_all, batch_size=32, shuffle=False)\n\n\n# We need a new model with 9 channels\n\nclass HAR1DCNN_9ch(nn.Module):\n    def __init__(self, in_channels=9, n_classes=6):  # adjust n_classes!\n        super().__init__()\n        self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm1d(32)\n        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.pool = nn.MaxPool1d(2)\n        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Linear(128, n_classes)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.pool(x)          # (N, 32, 20)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)          # (N, 64, 10)\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.global_pool(x).squeeze(-1)  # (N, 128)\n        return self.fc(x)\n\n\nTrain the new model\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device.type != \"cuda\":\n  device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\nn_classes = len(torch.unique(torch.tensor(y_train)))\nmodel_2 = HAR1DCNN_9ch(in_channels=9, n_classes=n_classes).to(device)\n\n\n# Optimizer and loss function\noptim = torch.optim.Adam(model_2.parameters(), lr=1e-3)\nloss_fn = nn.CrossEntropyLoss()\n\n\ndef run_epoch(loader, train=True):\n  if train:\n    model_2.train()\n  else:\n    model_2.eval()\n  total_loss, correct, total = 0, 0, 0\n  for Xb, yb in loader:\n    Xb, yb = Xb.to(device), yb.to(device)\n    if train:\n      optim.zero_grad()\n    out = model_2(Xb)\n    loss = loss_fn(out, yb.squeeze())\n    if train:\n      loss.backward()\n      optim.step()\n\n    total_loss += loss.item() * Xb.size(0)\n    preds = out.argmax(1)\n\n    correct += (preds == yb.squeeze()).sum().item()\n    total += Xb.size(0)\n  return total_loss/total, correct/total\n\n\nfor epoch in range(20):\n  train_loss, train_acc = run_epoch(train_dl_all)\n  test_loss, test_acc = run_epoch(test_dl_all, train=False)\n  print(f'Epoch {epoch+1}: train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, test_loss={test_loss:.4f}, test_acc={test_acc:.4f}')\n\nEpoch 1: train_loss=0.5655, train_acc=0.8259, test_loss=0.3217, test_acc=0.8975\nEpoch 2: train_loss=0.2185, train_acc=0.9227, test_loss=0.2600, test_acc=0.9138\nEpoch 3: train_loss=0.1702, train_acc=0.9358, test_loss=0.2306, test_acc=0.9182\nEpoch 4: train_loss=0.1505, train_acc=0.9382, test_loss=0.2432, test_acc=0.9209\nEpoch 5: train_loss=0.1351, train_acc=0.9434, test_loss=0.4624, test_acc=0.8364\nEpoch 6: train_loss=0.1360, train_acc=0.9421, test_loss=0.2184, test_acc=0.9274\nEpoch 7: train_loss=0.1262, train_acc=0.9461, test_loss=0.3418, test_acc=0.8931\nEpoch 8: train_loss=0.1253, train_acc=0.9472, test_loss=0.3280, test_acc=0.8992\nEpoch 9: train_loss=0.1221, train_acc=0.9498, test_loss=0.1982, test_acc=0.9430\nEpoch 10: train_loss=0.1175, train_acc=0.9505, test_loss=0.2267, test_acc=0.9287\nEpoch 11: train_loss=0.1103, train_acc=0.9517, test_loss=0.2051, test_acc=0.9444\nEpoch 12: train_loss=0.1071, train_acc=0.9539, test_loss=0.2660, test_acc=0.9118\nEpoch 13: train_loss=0.1105, train_acc=0.9527, test_loss=0.2171, test_acc=0.9447\nEpoch 14: train_loss=0.1035, train_acc=0.9551, test_loss=0.5525, test_acc=0.8527\nEpoch 15: train_loss=0.1295, train_acc=0.9495, test_loss=0.2448, test_acc=0.9335\nEpoch 16: train_loss=0.1072, train_acc=0.9547, test_loss=0.2114, test_acc=0.9298\nEpoch 17: train_loss=0.1041, train_acc=0.9540, test_loss=0.2010, test_acc=0.9430\nEpoch 18: train_loss=0.1066, train_acc=0.9548, test_loss=0.2100, test_acc=0.9396\nEpoch 19: train_loss=0.1008, train_acc=0.9561, test_loss=0.2304, test_acc=0.9104\nEpoch 20: train_loss=0.0980, train_acc=0.9588, test_loss=0.2317, test_acc=0.9454\n\n\n\n# Evaluate the model - get F1 score, confusion matrix\n\nfrom sklearn.metrics import f1_score, confusion_matrix\n\nmodel_2.eval()\ny_pred = []\ny_true = []\nwith torch.no_grad():\n    for Xb, yb in test_dl_all:\n        Xb, yb = Xb.to(device), yb.to(device)\n        out = model_2(Xb)\n        preds = out.argmax(1)\n        y_pred.extend(preds.cpu().numpy())\n        y_true.extend(yb.cpu().numpy())\n\nf1 = f1_score(y_true, y_pred, average='macro')\ncm = confusion_matrix(y_true, y_pred)\nprint(f'F1 score: {f1:.4f}')\n\n# PLot confusion matrix\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title(f'Confusion Matrix (F1 score= {f1:.4f})')\nplt.xticks([0.5, 1.5, 2.5, 3.5, 4.5, 5.5], ['Walking', 'Upstairs', 'Downstairs', 'Sitting', 'Standing', 'Laying'])\nplt.yticks([0.5, 1.5, 2.5, 3.5, 4.5, 5.5], ['Walking', 'Upstairs', 'Downstairs', 'Sitting', 'Standing', 'Laying]'])\nplt.show()\n\nF1 score: 0.9452\n\n\n\n\n\n\n\n\n\nAs we can see, this has improved on the accuracy on our first model (albeit, not by a lot - because our first model was already quite good)."
  }
]