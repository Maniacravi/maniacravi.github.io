[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my corner of the internet. Here, I write about whatever catches my curiosity — from random thoughts and philosophy to the science that shapes how we see the world.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/human-activity-detection/index.html",
    "href": "projects/human-activity-detection/index.html",
    "title": "Human Activity Detection using UCI’s HAR Dataset",
    "section": "",
    "text": "Okay, now we are stepping things up a little bit. This time, the data comes from UCI. According to the website:\nThe dataset is a bit different - it’s already been divided into train and test sets but they are all text files. Once again, let’s get into it - for fun, let’s also use the same model from last time to see if that architecture still works fine for this dataset. If it doesn’t, let’s see if we can build a different one."
  },
  {
    "objectID": "projects/human-activity-detection/index.html#load-data",
    "href": "projects/human-activity-detection/index.html#load-data",
    "title": "Human Activity Detection using UCI’s HAR Dataset",
    "section": "Load Data",
    "text": "Load Data\nThe data is split into total, body and gyro - these are the sensor values. There’s also another dataset that has some 561 features extracted from this. Might be fun to play with them but for now, let’s focus on building our CNN network - so this ‘total’ values would be fine for now. May play with .body’ accel values later.\n\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/total_acc_x_train.txt' # Update this path to your file location\n\n# Load into pandas\nimport pandas as pd\nimport re\n\ndata = pd.read_csv(file_location, sep='\\s+', header=None)\ndisplay(data.head())\ndata.shape\n\n&lt;&gt;:7: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:7: SyntaxWarning: invalid escape sequence '\\s'\n/var/folders/q7/ftl_yg4n1gndbwlnk654hq1h0000gn/T/ipykernel_55236/3576027909.py:7: SyntaxWarning: invalid escape sequence '\\s'\n  data = pd.read_csv(file_location, sep='\\s+', header=None)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n\n\n\n\n0\n1.012817\n1.022833\n1.022028\n1.017877\n1.023680\n1.016974\n1.017746\n1.019263\n1.016417\n1.020745\n...\n1.020981\n1.018065\n1.019638\n1.020017\n1.018766\n1.019815\n1.019290\n1.018445\n1.019372\n1.021171\n\n\n1\n1.018851\n1.022380\n1.020781\n1.020218\n1.021344\n1.020522\n1.019790\n1.019216\n1.018307\n1.017996\n...\n1.019291\n1.019258\n1.020736\n1.020950\n1.020491\n1.018685\n1.015660\n1.014788\n1.016499\n1.017849\n\n\n2\n1.023127\n1.021882\n1.019178\n1.015861\n1.012893\n1.016451\n1.020331\n1.020266\n1.021759\n1.018649\n...\n1.020304\n1.021516\n1.019417\n1.019312\n1.019448\n1.019434\n1.019916\n1.021041\n1.022935\n1.022019\n\n\n3\n1.017682\n1.018149\n1.019854\n1.019880\n1.019121\n1.020479\n1.020595\n1.016340\n1.010611\n1.009013\n...\n1.021295\n1.022934\n1.022183\n1.021637\n1.020598\n1.018887\n1.019161\n1.019916\n1.019602\n1.020735\n\n\n4\n1.019952\n1.019616\n1.020933\n1.023061\n1.022242\n1.020867\n1.021939\n1.022300\n1.022302\n1.022254\n...\n1.022687\n1.023670\n1.019899\n1.017381\n1.020389\n1.023884\n1.021753\n1.019425\n1.018896\n1.016787\n\n\n\n\n5 rows × 128 columns\n\n\n\n(7352, 128)\n\n\n\n# Let's see how the labels look like\n\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/y_train.txt'\n\n# Load\ndata = pd.read_csv(file_location, sep='\\s+', header=None)\ndisplay(data.head())\ndata.shape\n\n&lt;&gt;:6: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:6: SyntaxWarning: invalid escape sequence '\\s'\n/var/folders/q7/ftl_yg4n1gndbwlnk654hq1h0000gn/T/ipykernel_55236/3965665900.py:6: SyntaxWarning: invalid escape sequence '\\s'\n  data = pd.read_csv(file_location, sep='\\s+', header=None)\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n5\n\n\n1\n5\n\n\n2\n5\n\n\n3\n5\n\n\n4\n5\n\n\n\n\n\n\n\n(7352, 1)\n\n\nOkay looks like there are 7352 segments of windowed data with associated labels\n\nMake a Dataframe\n\n# Load X axis data\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/total_acc_x_train.txt'\ndf_total = pd.read_csv(file_location, sep='\\s+', header=None)\nfile_loc_body = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/body_acc_x_train.txt'\ndf_body = pd.read_csv(file_loc_body, sep='\\s+', header=None)\nfile_loc_gyro = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/body_gyro_x_train.txt'\ndf_gyro = pd.read_csv(file_loc_gyro, sep='\\s+', header=None)\n\n# Append Y axis data\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/total_acc_y_train.txt'\ndf_total = pd.concat([df_total, pd.read_csv(file_location, sep='\\s+', header=None)], axis=1)\nfile_loc_body = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/body_acc_y_train.txt'\ndf_body = pd.concat([df_body, pd.read_csv(file_loc_body, sep='\\s+', header=None)], axis=1)\nfile_loc_gyro = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/body_gyro_y_train.txt'\ndf_gyro = pd.concat([df_gyro, pd.read_csv(file_loc_gyro, sep='\\s+', header=None)], axis=1)\n\n# Z axis\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/total_acc_z_train.txt'\ndf_total = pd.concat([df_total, pd.read_csv(file_location, sep='\\s+', header=None)], axis=1)\nfile_loc_body = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/body_acc_z_train.txt'\ndf_body = pd.concat([df_body, pd.read_csv(file_loc_body, sep='\\s+', header=None)], axis=1)\nfile_loc_gyro = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/Inertial Signals/body_gyro_z_train.txt'\ndf_gyro = pd.concat([df_gyro, pd.read_csv(file_loc_gyro, sep='\\s+', header=None)], axis=1)\n\n# Labels\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/train/y_train.txt'\ny_train = pd.read_csv(file_location, sep='\\s+', header=None)\n# Subtract 1 from values\ny_train -= 1\n\n\ndf_total.shape, df_body.shape, y_train.shape\n\n((7352, 384), (7352, 384), (7352, 1))\n\n\n\n# Load into 3 dimensional numpy arrays\n\nimport numpy as np\n\nX_total_train = df_total.values.reshape(7352, 128, 3)\nX_body_train = df_body.values.reshape(7352, 128, 3)\nX_gyro_train = df_gyro.values.reshape(7352, 128, 3)\ny_train = np.array(y_train)\n\n\n# Do the same for test data\n\n# Load X axis data\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/total_acc_x_test.txt'\ndf_total_test = pd.read_csv(file_location, sep='\\s+', header=None)\nfile_loc_body = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/body_acc_x_test.txt'\ndf_body_test = pd.read_csv(file_loc_body, sep='\\s+', header=None)\nfile_loc_gyro = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/body_gyro_x_test.txt'\ndf_gyro = pd.read_csv(file_loc_gyro, sep='\\s+', header=None)\n\n# Append Y axis data\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/total_acc_y_test.txt'\ndf_total_test = pd.concat([df_total_test, pd.read_csv(file_location, sep='\\s+', header=None)], axis=1)\nfile_loc_body = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/body_acc_y_test.txt'\ndf_body_test = pd.concat([df_body_test, pd.read_csv(file_loc_body, sep='\\s+', header=None)], axis=1)\nfile_loc_gyro = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/body_gyro_y_test.txt'\ndf_gyro = pd.concat([df_gyro, pd.read_csv(file_loc_gyro, sep='\\s+', header=None)], axis=1)\n\n# Z axis\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/total_acc_z_test.txt'\ndf_total_test = pd.concat([df_total_test, pd.read_csv(file_location, sep='\\s+', header=None)], axis=1)\nfile_loc_body = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/body_acc_z_test.txt'\ndf_body_test = pd.concat([df_body_test, pd.read_csv(file_loc_body, sep='\\s+', header=None)], axis=1)\nfile_loc_gyro = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/Inertial Signals/body_gyro_z_test.txt'\ndf_gyro = pd.concat([df_gyro, pd.read_csv(file_loc_gyro, sep='\\s+', header=None)], axis=1)\n\n# Labels\nfile_location = '/Users/manikandanravi/Code/quarto-test/uci-har-dataset/test/y_test.txt'\ny_test = pd.read_csv(file_location, sep='\\s+', header=None)\ny_test -= 1\n\n# Convert to numpy arrays\nX_total_test = df_total_test.values.reshape(len(df_total_test), 128, 3)\nX_body_test = df_body_test.values.reshape(len(df_body_test), 128, 3)\nX_gyro_test = df_gyro.values.reshape(len(df_gyro), 128, 3)\ny_test = np.array(y_test)\n\nprint(\"Shapes of test data:\")\nprint(\"X_total_test:\", X_total_test.shape)\nprint(\"X_body_test:\", X_body_test.shape)\nprint(\"X_gyro_test:\", X_gyro_test.shape)\nprint(\"y_test:\", y_test.shape)\n\nOKay, now we have the data. Let’s get to work on building dataloaders and the models"
  },
  {
    "objectID": "projects/human-activity-detection/index.html#build-model",
    "href": "projects/human-activity-detection/index.html#build-model",
    "title": "Human Activity Detection using UCI’s HAR Dataset",
    "section": "Build Model",
    "text": "Build Model\n\nData Loader\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Dataset wrapper\nclass UCIDatasetWrapper(Dataset):\n  def __init__(self, X, y):\n    # Convert (N, T, C) -&gt; (N, C, T) for Conv1D\n    self.X = torch.tensor(X.transpose(0,2,1), dtype=torch.float32)\n    self.y = torch.tensor(y, dtype=torch.long)\n\n  def __len__(self):\n    return len(self.y)\n\n  def __getitem__(self, i):\n    return self.X[i], self.y[i]\n\n\n  train_ds = UCIDatasetWrapper(X_total_train, y_train)\n  train_ds_body = UCIDatasetWrapper(X_body_train, y_train)\n  test_ds = UCIDatasetWrapper(X_total_test, y_test)\n  test_ds_body = UCIDatasetWrapper(X_body_test, y_test)\n\n  train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n  train_dl_body = DataLoader(train_ds_body, batch_size=32, shuffle=True)\n  test_dl = DataLoader(test_ds, batch_size=32, shuffle=False)\n\n\n\nModel\nLets use the same model from the previous notbook\n\n# 1D CNN model (Same as our previous notebook)\nimport torch.nn.functional as F\nclass HAR1DCNN(nn.Module):\n    def __init__(self, in_channels=3, n_classes=6):  # adjust n_classes!\n        super().__init__()\n        self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm1d(32)\n        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.pool = nn.MaxPool1d(2)\n        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Linear(128, n_classes)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.pool(x)          # (N, 32, 20)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)          # (N, 64, 10)\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.global_pool(x).squeeze(-1)  # (N, 128)\n        return self.fc(x)"
  },
  {
    "objectID": "projects/human-activity-detection/index.html#train-the-model",
    "href": "projects/human-activity-detection/index.html#train-the-model",
    "title": "Human Activity Detection using UCI’s HAR Dataset",
    "section": "Train the model",
    "text": "Train the model\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device.type != \"cuda\":\n  device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n\nn_classes = len(torch.unique(torch.tensor(y_train)))\nmodel = HAR1DCNN(in_channels=3, n_classes=n_classes).to(device)\n\n\n# Optimizer and loss function\noptim = torch.optim.Adam(model.parameters(), lr=1e-3)\nloss_fn = nn.CrossEntropyLoss()\n\n\ndef run_epoch(loader, train=True):\n  if train:\n    model.train()\n  else:\n    model.eval()\n  total_loss, correct, total = 0, 0, 0\n  for Xb, yb in loader:\n    Xb, yb = Xb.to(device), yb.to(device)\n    if train:\n      optim.zero_grad()\n    out = model(Xb)\n    loss = loss_fn(out, yb.squeeze()) # Removed - 1\n    if train:\n      loss.backward()\n      optim.step()\n\n    total_loss += loss.item() * Xb.size(0)\n    preds = out.argmax(1)\n\n    correct += (preds == yb.squeeze()).sum().item() # Removed - 1\n    total += Xb.size(0)\n  return total_loss/total, correct/total\n\n\nfor epoch in range(10):\n  train_loss, train_acc = run_epoch(train_dl)\n  test_loss, test_acc = run_epoch(test_dl, train=False)\n  print(f'Epoch {epoch+1}: train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, test_loss={test_loss:.4f}, test_acc={test_acc:.4f}')\n\nEpoch 1: train_loss=0.5877, train_acc=0.8232, test_loss=0.3945, test_acc=0.8354\nEpoch 2: train_loss=0.2595, train_acc=0.9083, test_loss=0.3221, test_acc=0.8856\nEpoch 3: train_loss=0.2047, train_acc=0.9210, test_loss=0.2862, test_acc=0.8873\nEpoch 4: train_loss=0.1797, train_acc=0.9320, test_loss=0.2683, test_acc=0.8761\nEpoch 5: train_loss=0.1613, train_acc=0.9357, test_loss=0.3262, test_acc=0.8649\nEpoch 6: train_loss=0.1549, train_acc=0.9377, test_loss=0.2309, test_acc=0.9087\nEpoch 7: train_loss=0.1429, train_acc=0.9407, test_loss=0.2487, test_acc=0.9097\nEpoch 8: train_loss=0.1482, train_acc=0.9418, test_loss=0.2335, test_acc=0.9169\nEpoch 9: train_loss=0.1367, train_acc=0.9450, test_loss=0.2720, test_acc=0.9040\nEpoch 10: train_loss=0.1353, train_acc=0.9452, test_loss=0.2988, test_acc=0.8843"
  },
  {
    "objectID": "projects/human-activity-detection/index.html#evaluate-model",
    "href": "projects/human-activity-detection/index.html#evaluate-model",
    "title": "Human Activity Detection using UCI’s HAR Dataset",
    "section": "Evaluate model",
    "text": "Evaluate model\n\n# Evaluate the model - get F1 score, confusion matrix\n\nfrom sklearn.metrics import f1_score, confusion_matrix\n\nmodel.eval()\ny_pred = []\ny_true = []\nwith torch.no_grad():\n    for Xb, yb in test_dl:\n        Xb, yb = Xb.to(device), yb.to(device)\n        out = model(Xb)\n        preds = out.argmax(1)\n        y_pred.extend(preds.cpu().numpy())\n        y_true.extend(yb.cpu().numpy())\n\nf1 = f1_score(y_true, y_pred, average='macro')\ncm = confusion_matrix(y_true, y_pred)\nprint(f'F1 score: {f1:.4f}')\n\n# PLot confusion matrix\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title(f'Confusion Matrix (F1 score= {f1:.4f})')\nplt.xticks([0.5, 1.5, 2.5, 3.5, 4.5, 5.5], ['Walking', 'Upstairs', 'Downstairs', 'Sitting', 'Standing', 'Laying'])\nplt.yticks([0.5, 1.5, 2.5, 3.5, 4.5, 5.5], ['Walking', 'Upstairs', 'Downstairs', 'Sitting', 'Standing', 'Laying]'])\nplt.show()\n\nF1 score: 0.8820"
  },
  {
    "objectID": "projects/human-activity-detection/index.html#changing-data",
    "href": "projects/human-activity-detection/index.html#changing-data",
    "title": "Human Activity Detection using UCI’s HAR Dataset",
    "section": "Changing Data",
    "text": "Changing Data\nWhat if we train the model just with the ‘body’ accelerometer data?\nJust to recap: &gt; The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.\n\nfor epoch in range(10):\n  train_loss, train_acc = run_epoch(train_dl_body)\n  test_loss, test_acc = run_epoch(test_dl, train=False)\n  print(f'Epoch {epoch+1}: train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, test_loss={test_loss:.4f}, test_acc={test_acc:.4f}')\n\nEpoch 1: train_loss=0.7459, train_acc=0.6017, test_loss=5.3111, test_acc=0.4411\nEpoch 2: train_loss=0.6667, train_acc=0.6245, test_loss=5.0896, test_acc=0.3006\nEpoch 3: train_loss=0.6550, train_acc=0.6304, test_loss=7.9090, test_acc=0.2694\nEpoch 4: train_loss=0.6306, train_acc=0.6383, test_loss=9.0907, test_acc=0.1968\nEpoch 5: train_loss=0.6254, train_acc=0.6510, test_loss=10.1547, test_acc=0.2158\nEpoch 6: train_loss=0.6231, train_acc=0.6508, test_loss=10.2332, test_acc=0.2063\nEpoch 7: train_loss=0.6294, train_acc=0.6502, test_loss=12.2881, test_acc=0.2172\nEpoch 8: train_loss=0.6161, train_acc=0.6564, test_loss=16.7289, test_acc=0.1849\nEpoch 9: train_loss=0.6094, train_acc=0.6585, test_loss=16.9381, test_acc=0.1822\nEpoch 10: train_loss=0.6113, train_acc=0.6586, test_loss=18.3700, test_acc=0.1822\n\n\n\n# Evaluate the model - get F1 score, confusion matrix\n\nfrom sklearn.metrics import f1_score, confusion_matrix\n\nmodel.eval()\ny_pred = []\ny_true = []\nwith torch.no_grad():\n    for Xb, yb in test_dl:\n        Xb, yb = Xb.to(device), yb.to(device)\n        out = model(Xb)\n        preds = out.argmax(1)\n        y_pred.extend(preds.cpu().numpy())\n        y_true.extend(yb.cpu().numpy())\n\nf1 = f1_score(y_true, y_pred, average='macro')\ncm = confusion_matrix(y_true, y_pred)\nprint(f'F1 score: {f1:.4f}')\n\n# PLot confusion matrix\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title(f'Confusion Matrix (F1 score= {f1:.4f})')\nplt.xticks([0.5, 1.5, 2.5, 3.5, 4.5, 5.5], ['Walking', 'Upstairs', 'Downstairs', 'Sitting', 'Standing', 'Laying'])\nplt.yticks([0.5, 1.5, 2.5, 3.5, 4.5, 5.5], ['Walking', 'Upstairs', 'Downstairs', 'Sitting', 'Standing', 'Laying]'])\nplt.show()\n\nF1 score: 0.0514\n\n\n\n\n\n\n\n\n\nAs we can see (and intutitively expect), if we take out the gravitational component, the accuracy values are pretty poor. What if we feed all these 9 vectors in?"
  },
  {
    "objectID": "projects/human-activity-detection/index.html#running-the-model-with-all-the-data",
    "href": "projects/human-activity-detection/index.html#running-the-model-with-all-the-data",
    "title": "Human Activity Detection using UCI’s HAR Dataset",
    "section": "Running the model with all the data",
    "text": "Running the model with all the data\n\n# Concatenate total, body and gyro\n\nX_all_train = np.concat((X_total_train, X_body_train, X_gyro_train), axis=2)\ndisplay(X_all_train.shape, y_train.shape) # We should have 9 channels now\nX_all_test = np.concat((X_total_test, X_body_test, X_gyro_test), axis=2)\ndisplay(X_all_test.shape, y_test.shape)\n\n(7352, 128, 9)\n\n\n(7352, 1)\n\n\n(2947, 128, 9)\n\n\n(2947, 1)\n\n\n\n# Create data loaders\n\ntrain_ds_all = UCIDatasetWrapper(X_all_train, y_train)\ntrain_dl_all = DataLoader(train_ds_all, batch_size=32, shuffle=True)\ntest_ds_all = UCIDatasetWrapper(X_all_test, y_test)\ntest_dl_all = DataLoader(test_ds_all, batch_size=32, shuffle=False)\n\n\n# We need a new model with 9 channels\n\nclass HAR1DCNN_9ch(nn.Module):\n    def __init__(self, in_channels=9, n_classes=6):  # adjust n_classes!\n        super().__init__()\n        self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm1d(32)\n        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.pool = nn.MaxPool1d(2)\n        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm1d(128)\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Linear(128, n_classes)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.pool(x)          # (N, 32, 20)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.pool(x)          # (N, 64, 10)\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.global_pool(x).squeeze(-1)  # (N, 128)\n        return self.fc(x)\n\n\nTrain the new model\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device.type != \"cuda\":\n  device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\nn_classes = len(torch.unique(torch.tensor(y_train)))\nmodel_2 = HAR1DCNN_9ch(in_channels=9, n_classes=n_classes).to(device)\n\n\n# Optimizer and loss function\noptim = torch.optim.Adam(model_2.parameters(), lr=1e-3)\nloss_fn = nn.CrossEntropyLoss()\n\n\ndef run_epoch(loader, train=True):\n  if train:\n    model_2.train()\n  else:\n    model_2.eval()\n  total_loss, correct, total = 0, 0, 0\n  for Xb, yb in loader:\n    Xb, yb = Xb.to(device), yb.to(device)\n    if train:\n      optim.zero_grad()\n    out = model_2(Xb)\n    loss = loss_fn(out, yb.squeeze())\n    if train:\n      loss.backward()\n      optim.step()\n\n    total_loss += loss.item() * Xb.size(0)\n    preds = out.argmax(1)\n\n    correct += (preds == yb.squeeze()).sum().item()\n    total += Xb.size(0)\n  return total_loss/total, correct/total\n\n\nfor epoch in range(20):\n  train_loss, train_acc = run_epoch(train_dl_all)\n  test_loss, test_acc = run_epoch(test_dl_all, train=False)\n  print(f'Epoch {epoch+1}: train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, test_loss={test_loss:.4f}, test_acc={test_acc:.4f}')\n\nEpoch 1: train_loss=0.5696, train_acc=0.8290, test_loss=0.3161, test_acc=0.8901\nEpoch 2: train_loss=0.2113, train_acc=0.9245, test_loss=0.2400, test_acc=0.9111\nEpoch 3: train_loss=0.1720, train_acc=0.9323, test_loss=0.2616, test_acc=0.9050\nEpoch 4: train_loss=0.1541, train_acc=0.9378, test_loss=0.5100, test_acc=0.8168\nEpoch 5: train_loss=0.1456, train_acc=0.9404, test_loss=0.2445, test_acc=0.9175\nEpoch 6: train_loss=0.1269, train_acc=0.9461, test_loss=0.2456, test_acc=0.9192\nEpoch 7: train_loss=0.1234, train_acc=0.9468, test_loss=0.2176, test_acc=0.9298\nEpoch 8: train_loss=0.1274, train_acc=0.9463, test_loss=0.2412, test_acc=0.9057\nEpoch 9: train_loss=0.1156, train_acc=0.9504, test_loss=0.2075, test_acc=0.9335\nEpoch 10: train_loss=0.1155, train_acc=0.9506, test_loss=0.2143, test_acc=0.9325\nEpoch 11: train_loss=0.1161, train_acc=0.9506, test_loss=0.2402, test_acc=0.9247\nEpoch 12: train_loss=0.1132, train_acc=0.9510, test_loss=0.2097, test_acc=0.9318\nEpoch 13: train_loss=0.1122, train_acc=0.9504, test_loss=0.2070, test_acc=0.9416\nEpoch 14: train_loss=0.1088, train_acc=0.9551, test_loss=0.2278, test_acc=0.9386\nEpoch 15: train_loss=0.1059, train_acc=0.9561, test_loss=0.2006, test_acc=0.9430\nEpoch 16: train_loss=0.1069, train_acc=0.9547, test_loss=0.3617, test_acc=0.8761\nEpoch 17: train_loss=0.1043, train_acc=0.9559, test_loss=0.2004, test_acc=0.9460\nEpoch 18: train_loss=0.1017, train_acc=0.9570, test_loss=0.2802, test_acc=0.9138\nEpoch 19: train_loss=0.1041, train_acc=0.9566, test_loss=0.2089, test_acc=0.9437\nEpoch 20: train_loss=0.1043, train_acc=0.9539, test_loss=0.2495, test_acc=0.9145\n\n\n\n# Evaluate the model - get F1 score, confusion matrix\n\nfrom sklearn.metrics import f1_score, confusion_matrix\n\nmodel_2.eval()\ny_pred = []\ny_true = []\nwith torch.no_grad():\n    for Xb, yb in test_dl_all:\n        Xb, yb = Xb.to(device), yb.to(device)\n        out = model_2(Xb)\n        preds = out.argmax(1)\n        y_pred.extend(preds.cpu().numpy())\n        y_true.extend(yb.cpu().numpy())\n\nf1 = f1_score(y_true, y_pred, average='macro')\ncm = confusion_matrix(y_true, y_pred)\nprint(f'F1 score: {f1:.4f}')\n\n# PLot confusion matrix\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title(f'Confusion Matrix (F1 score= {f1:.4f})')\nplt.xticks([0.5, 1.5, 2.5, 3.5, 4.5, 5.5], ['Walking', 'Upstairs', 'Downstairs', 'Sitting', 'Standing', 'Laying'])\nplt.yticks([0.5, 1.5, 2.5, 3.5, 4.5, 5.5], ['Walking', 'Upstairs', 'Downstairs', 'Sitting', 'Standing', 'Laying]'])\nplt.show()\n\nF1 score: 0.9128\n\n\n\n\n\n\n\n\n\nAs we can see, this has improved on the accuracy on our first model (albeit, not by a lot - because our first model was already quite good)."
  },
  {
    "objectID": "disclaimer/index.html",
    "href": "disclaimer/index.html",
    "title": "Mani Ravi",
    "section": "",
    "text": "The views and opinions expressed on this website are my own and do not necessarily reflect the official policy or position of any current or past employers, collaborators, or affiliated institutions — including the Kessler Foundation, University of Louisville, or any research partners.\nAll project descriptions, analyses, and visualizations presented here are for educational and illustrative purposes only. They do not represent proprietary or confidential work performed under employment or contract."
  },
  {
    "objectID": "disclaimer/index.html#disclaimer",
    "href": "disclaimer/index.html#disclaimer",
    "title": "Mani Ravi",
    "section": "",
    "text": "The views and opinions expressed on this website are my own and do not necessarily reflect the official policy or position of any current or past employers, collaborators, or affiliated institutions — including the Kessler Foundation, University of Louisville, or any research partners.\nAll project descriptions, analyses, and visualizations presented here are for educational and illustrative purposes only. They do not represent proprietary or confidential work performed under employment or contract."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "A collection of projects exploring the intersection of data science, bioengineering, and AI-driven sensing — with links to code and demos where available.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHuman Activity Detection using UCI’s HAR Dataset\n\n\n\nIMU\n\nGait\n\nHuman Activity\n\nMachine Learning\n\nCNN\n\n\n\nClassifying Accelerometer data to various activities of daily living (ADL) using Deep Learning.\n\n\n\n\n\nSep 25, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Manikandan (Mani) Ravi",
    "section": "",
    "text": "I’m a Biomedical Engineer and Data Scientist working on, and enthusiastic about, the intersection of Humans, Sensors and AI. I’ve spent over a decade developing algorithms that translate complex physiological signals - from EMG and ECG to PPG and IMU data - into meaningful, actionable information.\nI currently work at Kessler Foundation’s Center for Spinal Stimulation, leading biomedical signal processing and machine learning projects focused on Neuromodulation and spinal cord injury research.\nMy long-term goal is to build technologies that make physiological data more actionable and interpretable. I’m endlessly curious about how technology can deepen our understanding of the body — the signals we measure tell stories about how we move, sleep, think, and even feel.\n\n“Nobody ever figures out what life is all about, and it doesn’t matter. Explore the world. Nearly everything is really interesting if you go into it deeply enough.” - Richard P. Feynman"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Manikandan (Mani) Ravi",
    "section": "",
    "text": "I’m a Biomedical Engineer and Data Scientist working on, and enthusiastic about, the intersection of Humans, Sensors and AI. I’ve spent over a decade developing algorithms that translate complex physiological signals - from EMG and ECG to PPG and IMU data - into meaningful, actionable information.\nI currently work at Kessler Foundation’s Center for Spinal Stimulation, leading biomedical signal processing and machine learning projects focused on Neuromodulation and spinal cord injury research.\nMy long-term goal is to build technologies that make physiological data more actionable and interpretable. I’m endlessly curious about how technology can deepen our understanding of the body — the signals we measure tell stories about how we move, sleep, think, and even feel.\n\n“Nobody ever figures out what life is all about, and it doesn’t matter. Explore the world. Nearly everything is really interesting if you go into it deeply enough.” - Richard P. Feynman"
  },
  {
    "objectID": "index.html#key-interests",
    "href": "index.html#key-interests",
    "title": "Manikandan (Mani) Ravi",
    "section": "Key Interests",
    "text": "Key Interests\n\nBiomedical signal processing\nWearables & Digital Health\nMachine Learning and Deep Learning\nBrain-Computer Interfaces (BCI)\nNeuromodulation and Neuroengineering"
  },
  {
    "objectID": "index.html#personal-life",
    "href": "index.html#personal-life",
    "title": "Manikandan (Mani) Ravi",
    "section": "Personal Life",
    "text": "Personal Life\nI live in New Jersey where I split my time between work, side projects, playing badminton, watching physics lectures, reading philosophy books and catching up on sports - basketball, soccer and test cricket (yes, it’s 5 days long but there’s nothing quite like it!)."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Manikandan (Mani) Ravi",
    "section": "Contact",
    "text": "Contact\nIf you find something in here interesting or if you want to chat with me about something, please drop a message (or a connection request) on LinkedIn!"
  }
]